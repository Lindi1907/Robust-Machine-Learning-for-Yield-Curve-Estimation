

\section{Gaussian Process and Bayesian Perspective}
After estimating the discount function $g(\boldsymbol{x})$, we want to do inference with this function, and because of the nonparametric estimation, we cannot calculate the statistical distribution of parameters, hence here we use Gaussian Process to estimate the distribution of discount function and construct a confidence interval. 
\subsection{Definition}
We have data $\textbf{D} = \{ (\boldsymbol{x_i}, y_i) \} _{i=1} ^{M}$, and assume that mean of y is 0. 
\\ \\
Task: find the distribution of $ f^{*}(x) $.  
\\ \\
Assume that the true form of prediction function is: $y_i = f(\boldsymbol{x_i}) + \epsilon_i$ and $\epsilon_i \sim \mathcal{N}(0,\sigma_{i}^{2})$. Here we have an M dimensional dependent variable \textbf{y}, and a $M \times N$ dimensional independent variable \textbf{X}, where M is the number of observations, and N is the dimension of x, i.e. $\boldsymbol{x_i}\in \mathbb{R}^N $. The function 
$f(\boldsymbol{x_i}) : \mathbb{R}^N \to \mathbb{R}$ takes vector $\boldsymbol{x_i} \in \mathbb{R}^N$. Let $\boldsymbol{K}_{X, X} = k(\boldsymbol{x},\boldsymbol{x}^T)$ which is the matrix of $k(\boldsymbol{x_i}, \boldsymbol{x_j})$. Thus, \textbf{K} is a $M \times M$ matrix. 
\\ \\
The assumption of the Gaussian Process is as follows: \\ \\
For a given vector \textbf{y}, and its corresponding data \textbf{X}, where vector $ \boldsymbol{y} \in \mathbb{R}^M$ and $\boldsymbol{X}$ is $M \times N$ matrix. In addition, for \textbf{y} and \textbf{X} data, the error term $\epsilon \sim \mathcal{N}(\boldsymbol{0},\varSigma^{\epsilon})$, and $\varSigma^{\epsilon} = diag(\sigma_{1}^{2}, \sigma_{2}^{2}, \sigma_{3}^{2}, ......, \sigma_{M}^{2}) $.  Meanwhile we have arbitrary $n \times N$ matrix \textbf{Z} and predicted value $f^{*}(\boldsymbol{z}) \in \mathbb{R}^n$, where $ \boldsymbol{z} = (\boldsymbol{z_1, z_2, z_3, ......, z_n})^T$. \\ \\
Then we assume \textbf{y} and $f^{*}(\boldsymbol{z})$ follow a $(M + n)$ multivariate normal distribution(MVN): \\
\begin{equation}
\begin{bmatrix}
f^{*}(\boldsymbol{z}) \\
\boldsymbol{y} 
\end{bmatrix} \sim \mathcal{N}
\begin{pmatrix}
\begin{bmatrix}
\mu_{f^{*}(\boldsymbol{z})} \\ \mu_{\boldsymbol{y}}
\end{bmatrix} 
 & ,\begin{bmatrix}
\boldsymbol{K}_{Z, Z} & \boldsymbol{K}_{Z, X} \\
\boldsymbol{K}_{X, Z} & \hat{\boldsymbol{K}}_{X, X}
\end{bmatrix}
\end{pmatrix}
\end{equation}
where $ \hat{\boldsymbol{K}}_{X, X} = \boldsymbol{K}_{X, X} + \varSigma^{\epsilon}$. \\ \\
Then given data \textbf{y}, \textbf{X} and \textbf{Z}, according to the conditional distributions of the multivariate normal distribution\footnote{https://statproofbook.github.io/P/mvn-cond}, we have the posterior distribution 
\begin{equation}
 f^{*}(\boldsymbol{z}) | \boldsymbol{y}, \boldsymbol{X},  \boldsymbol{Z} \sim  \mathcal{N}(\mu_{f^{*}(\boldsymbol{z})} + \boldsymbol{K}_{Z, X}\hat{\boldsymbol{K}}_{X, X}^{-1}(\boldsymbol{y} - \mu_{\boldsymbol{y}}), \boldsymbol{K}_{Z, Z} - \boldsymbol{K}_{Z, X}\hat{\boldsymbol{K}}_{X, X}^{-1}\boldsymbol{K}_{X, Z})
\end{equation}

\subsection{Intuition behind Gaussian Process}
The idea behind this process is that, assume our interested function is $f(x)$, $ f(\boldsymbol{x}): \mathbb{R}^N \to \mathbb{R}$, and we have an arbitrary vector of independent variable $\boldsymbol{x} = (\boldsymbol{x_1}, \boldsymbol{x_2,} ......, \boldsymbol{x_M})^T$, and for each $\boldsymbol{x_i}, i = 1, 2, ...,M, x_i \in \mathbb{R}^N$, then we can obtain a series of $f(\boldsymbol{x})= (f(\boldsymbol{x_1}), f(\boldsymbol{x_2}), f(\boldsymbol{x_3}), ......, f(\boldsymbol{x_M}) )^T$. We assume that the series of f(\textbf{x}) follows a multivariate normal distribution which is: 

\begin{equation}
f(\boldsymbol{x}) \sim \mathcal{N}(\mu(\boldsymbol{x}), k(\boldsymbol{x},\boldsymbol{x}^T)) 
\end{equation}
\\ 
This is the prior distribution of our function $f(x)$, here we have a set of infinite functions that follow this distribution, their mean is the function $\mu(\boldsymbol{x_i})$, and the variance of them is $k(\boldsymbol{x_i},\boldsymbol{x_i}^T)$. This makes the distribution of $f(\boldsymbol{x})$ to be called Gaussian Process (GP). Note that if we add a noise term $\epsilon \sim \mathcal{N}(\boldsymbol{0},\varSigma^{\epsilon})$, then our prior distribution of $y = f(\boldsymbol{x}) + \epsilon \sim \mathcal{N}(\mu(\boldsymbol{x}), k(\boldsymbol{x},\boldsymbol{x}^T) + \varSigma^{\epsilon})$ is also a Gaussian Process. Here we use the kernel matrix to denote the variance-covariance matrix because the kernel value represents how near two data points in the space are, with this property we can obtain a smooth function.  \\ \\
Remind that our goal is to estimate the distribution of $f(\boldsymbol{x^*})$ given observed training data set $D = \{\boldsymbol{x_i}, y_i\}_{i = 1}^M$ and test data set $\{\boldsymbol{x_j^*}\}_{j = 1}^n$. Firstly we compare our nonparametric case to a parametric case. In a parametric case, assume the parameter $\theta$ determines the form of $f_{\theta}(\cdot)$, according to the Bayesian rule, 
$
p(\boldsymbol{y^*} | \boldsymbol{x^*}, \boldsymbol{x}, \boldsymbol{y}) = \int_{\theta} p(\boldsymbol{y^*}, \theta |  \boldsymbol{x^*}, \boldsymbol{x}, \boldsymbol{y})d\theta = \int_{\theta} p(\boldsymbol{y^*} | \theta, \boldsymbol{x^*})p(\theta | \boldsymbol{x}, \boldsymbol{y})d\theta
$
, where $\boldsymbol{y^*}$ is the prediction of given data $\boldsymbol{x^*}$, and its form of model is determmined by paramater $\theta$. The estimated $\theta$ value is determined by training data $D$. This is to say that we update our parameter $\theta$ by given $D$, and use $p(\theta |  \boldsymbol{x}, \boldsymbol{y})$ as a new prior probability, and based on this to predict posterior of $\boldsymbol{y^*}$.  \\ \\
Therefore, back to our GP nonparametric case, $\theta$ could be substituted by function $f(\cdot)$. One can show that the joint distribution of $(f(\boldsymbol{x^*}), \boldsymbol{y})^T$ follows a multivariate normal distribution as in the definition before, because of the assumption of GP and the property of MVN. With the joint distribution, we want to find posterior probability:  $p(f(\boldsymbol{x^*}) | \boldsymbol{x^*}, \boldsymbol{x}, \boldsymbol{y}) = \int p(f(\boldsymbol{x^*})  | f, \boldsymbol{x^*})p(f | \boldsymbol{x}, \boldsymbol{y})df$, where $p(f | \boldsymbol{x}, \boldsymbol{y})$ is the posterior of $f(\cdot)$ given $D$, and is regarded as prior when estimating $p(f(\boldsymbol{x^*}) | \boldsymbol{x^*}, D)$, this process is called Bayesian updating. Fortunately, we do not need to take any integral in GP, because the posterior of $f(\boldsymbol{x^*})$ could be calculated by the formula of conditional distribution in MVN as mentioned in the former section. 

\subsection{Gaussian Process in research paper}
In this research paper, authors assumed discount function $g(\boldsymbol{z})$ given a vector of different maturties $\boldsymbol{z} = (z_1, z_2, ......, z_n)$ follows a MVN distribution $\mathcal{N}(m(\boldsymbol{z}), k(\boldsymbol{z}, \boldsymbol{z}^T))$. Then this is a Gaussian Process, and by Bayesian updating for given price $P$, corresponding cash flow matrix $C$, and time to maturities $x$, we can obtain the posterior mean and variance function in the research paper's equation (12) and equation (13). Therefore, the variance function of MVN could give us the confidence interval of $g(z)$ i.e. for each maturity time $z$ we calculate $k^{post}(z,z)$ as its normal variance, which could help us to evaluate the precision of our prediction. Furthermore, with the posterior distribution of $g(\boldsymbol{x})$, it is implied that the coupon bond price $Cg(\boldsymbol{x}) \sim \mathcal{N}(Cm^{post}(\boldsymbol{x}), Ck^{post}(\boldsymbol{x},\boldsymbol{x}^T)C^T)$. \\ \\
Note that authors assume the variance covariance matrix of error term $\varSigma^{\epsilon} = diag(\sigma_{1}^{2}, \sigma_{2}^{2}, \sigma_{3}^{2}, ......, \sigma_{M}^{2}) $ where diagnoal elements all satisify $\omega_i = \frac{\lambda}{\sigma_{i}^{2}}$, this implies that we give a higher weight for a bond price which has less noise. In addition, we assume that the prior mean function is constant $m(x) = 1$ which assumes no time value of money. With these assumptions, the posterior mean function coincides with estimated $\hat{g}(\boldsymbol{x})$ as in the research paper's equation (5). 



